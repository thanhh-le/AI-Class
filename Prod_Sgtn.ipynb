{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanhh-le/AI-Class/blob/main/Prod_Sgtn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install prerequisite packages\n",
        "!pip install -q \"jedi>=0.16\"\n",
        "!pip -q install python-dotenv==1.0.1\n",
        "!pip -q install llama-index-llms-gemini llama-index\n",
        "!pip -q install google-genai\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import numpy as np, os"
      ],
      "metadata": {
        "id": "rQce0mqZAIzL",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d3a276-4c25-491e-c8c5-1982f7de8a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --force-reinstall --no-deps \\\n",
        "  \"langchain-core==0.3.67\" \"langsmith==0.2.10\" \\\n",
        "  \"langchain==0.3.12\" \"langchain-community==0.3.12\" \\\n",
        "  \"langchain-google-genai==2.1.12\" \"langchain-chroma==0.1.4\" \\\n",
        "  \"tenacity==9.0.0\" \"pysqlite3-binary==0.5.4\" \\\n",
        "  \"pandas==2.2.3\" \"pypdf==5.1.0\" \"nbformat==5.10.4\"\n",
        "\n",
        "    #\"langgraph==0.2.59\" \\"
      ],
      "metadata": {
        "id": "menonNyp4EDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fce91d4-417b-4986-a4d5-2e6bea1f31aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m440.2/440.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y langgraph langgraph-prebuilt\n",
        "!pip install -q \"langgraph==0.2.74\"\n",
        "\n",
        "# verify\n",
        "import langgraph, pkgutil\n",
        "print(\"subpackages:\", [m.name for m in pkgutil.iter_modules(langgraph.__path__)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063dO9cIRAdq",
        "outputId": "876a3ae7-c265-4be0-f475-8447d86284b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping langgraph as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langgraph-prebuilt as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m397.3/397.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-chroma 0.1.4 requires chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0, which is not installed.\n",
            "langchain-community 0.3.12 requires langsmith<0.3,>=0.1.125, but you have langsmith 0.4.38 which is incompatible.\n",
            "langchain-chroma 0.1.4 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.12 requires langsmith<0.3,>=0.1.17, but you have langsmith 0.4.38 which is incompatible.\n",
            "langchain-text-splitters 0.3.11 requires langchain-core<2.0.0,>=0.3.75, but you have langchain-core 0.3.67 which is incompatible.\n",
            "google-adk 1.16.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0msubpackages: ['_api', 'channels', 'config', 'constants', 'errors', 'func', 'graph', 'managed', 'prebuilt', 'pregel', 'types', 'utils', 'version']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Models"
      ],
      "metadata": {
        "id": "hno0FMNR-271"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pezoEzpA9ZQ1"
      },
      "outputs": [],
      "source": [
        "# Configure the API key from Colab's Secrets Manager\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "embedding = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",   # Geminiâ€™s embedding model\n",
        "    task_type=\"SEMANTIC_SIMILARITY\" # or RETRIEVAL_QUERY / RETRIEVAL_DOCUMENT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Use the new Gemini 2.5 Flash model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",   # ğŸ‘ˆ hereâ€™s the change\n",
        "    temperature=0.7,            # optional\n",
        "    max_output_tokens=1024      # optional\n",
        ")\n"
      ],
      "metadata": {
        "id": "uckb9Z5vQEYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add Product Pricing function tool"
      ],
      "metadata": {
        "id": "O1SEjWFg-6l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j42HTZyIXMC",
        "outputId": "5d770982-9b57-453a-bd6a-8242f05bf7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "#Load the laptop product pricing CSV into a Pandas dataframe.\n",
        "product_pricing_df = pd.read_csv(\"/content/drive/Shareddrives/118S Group Project/Laptop pricing.csv\")\n",
        "print(product_pricing_df)\n",
        "\n",
        "@tool\n",
        "def get_laptop_price(laptop_name:str) -> int :\n",
        "    \"\"\"\n",
        "    This function returns the price of a laptop, given its name as input.\n",
        "    It performs a substring match between the input name and the laptop name.\n",
        "    If a match is found, it returns the pricxe of the laptop.\n",
        "    If there is NO match found, it returns -1\n",
        "    \"\"\"\n",
        "\n",
        "    #Filter Dataframe for matching names\n",
        "    match_records_df = product_pricing_df[\n",
        "                        product_pricing_df[\"Name\"].str.contains(\n",
        "                                                \"^\" + laptop_name, case=False)\n",
        "                        ]\n",
        "    #Check if a record was found, if not return -1\n",
        "    if len(match_records_df) == 0 :\n",
        "        return -1\n",
        "    else:\n",
        "        return match_records_df[\"Price\"].iloc[0]\n",
        "\n",
        "#Test the tool. Before running the test, comment the @tool annotation\n",
        "#print(get_laptop_price(\"alpha\"))\n",
        "#print(get_laptop_price(\"testing\"))\n"
      ],
      "metadata": {
        "id": "S0ltdHo--iIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ac8384-d6bb-4483-9bd8-6165966b6990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Name  Price  ShippingDays\n",
            "0  AlphaBook Pro   1499             2\n",
            "1     GammaAir X   1399             7\n",
            "2  SpectraBook S   2499             7\n",
            "3   OmegaPro G17   2199            14\n",
            "4  NanoEdge Flex   1699             2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Product Features Retrieval Tool"
      ],
      "metadata": {
        "id": "Sz7rz9y7-9hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- install once per runtime ---\n",
        "!pip -q install faiss-cpu\n",
        "\n",
        "# --- imports ---\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# If you don't already have an embedding object defined, uncomment this:\n",
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "# embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "\n",
        "# Load, chunk and index the contents of the product features document.\n",
        "loader = PyPDFLoader(\"/content/drive/Shareddrives/118S Group Project/Laptop product descriptions.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Create a vector store with FAISS (drop-in replacement)\n",
        "prod_feature_store = FAISS.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding   # your existing Gemini embeddings object\n",
        ")\n",
        "\n",
        "# Build a retriever tool\n",
        "get_product_features = create_retriever_tool(\n",
        "    prod_feature_store.as_retriever(search_kwargs={\"k\": 1}),\n",
        "    name=\"Get_Product_Features\",\n",
        "    description=\"\"\"\n",
        "    This store contains details about Laptops. It lists the available laptops\n",
        "    and their features including CPU, memory, storage, design and advantages\n",
        "    \"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "jtR_XY97I4SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup a Product QnA chatbot"
      ],
      "metadata": {
        "id": "43KQLU02-_ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal rolling memory for each thread_id\n",
        "from collections import defaultdict\n",
        "\n",
        "class RollingMemory:\n",
        "    def __init__(self, max_turns: int = 12):\n",
        "        self.store = defaultdict(list)\n",
        "        self.max_turns = max_turns\n",
        "\n",
        "    def append(self, thread_id: str, role: str, content: str):\n",
        "        msgs = self.store[thread_id]\n",
        "        msgs.append((role, content))\n",
        "        # keep the last N turns (role, content pairs)\n",
        "        if len(msgs) > self.max_turns * 2:\n",
        "            self.store[thread_id] = msgs[-self.max_turns*2:]\n",
        "\n",
        "    def history(self, thread_id: str):\n",
        "        return self.store.get(thread_id, [])\n",
        "\n",
        "memory = RollingMemory(max_turns=12)\n"
      ],
      "metadata": {
        "id": "Dzt2_-r0OrQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_prompt = SystemMessage(\n",
        "    content=(\n",
        "        \"You are a professional chatbot that answers questions about laptops sold by our company. \"\n",
        "        \"Use the provided tools for facts; do not invent specs.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# no native checkpointer (since it's missing)\n",
        "checkpointer = None\n",
        "tools = [get_laptop_price, get_product_features]\n",
        "\n",
        "product_QnA_agent = create_react_agent(\n",
        "    model=model,      # your ChatGoogleGenerativeAI(...) or other LC LLM\n",
        "    tools=tools,      # [get_laptop_price, get_product_features]\n",
        "    state_modifier=system_prompt,\n",
        "    debug=False,\n",
        "    checkpointer=checkpointer,\n",
        ")\n",
        "\n",
        "# Helper to invoke with our own memory\n",
        "def invoke_with_memory(agent, user_text: str, thread_id: str = \"u1\"):\n",
        "    # prepend rolling history to this call\n",
        "    msgs = []\n",
        "    for role, content in memory.history(thread_id):\n",
        "        msgs.append((role, content))\n",
        "    msgs.append((\"user\", user_text))\n",
        "\n",
        "    resp = agent.invoke({\"messages\": msgs})\n",
        "    # append user + assistant to our memory\n",
        "    memory.append(thread_id, \"user\", user_text)\n",
        "    memory.append(thread_id, \"assistant\", resp[\"messages\"][-1].content)\n",
        "    return resp\n",
        "\n",
        "# Example usage\n",
        "r1 = invoke_with_memory(product_QnA_agent, \"What laptops are under $800?\", \"user-123\")\n",
        "print(r1[\"messages\"][-1].content)\n",
        "\n",
        "r2 = invoke_with_memory(product_QnA_agent, \"Prefer something lightweight with long battery.\", \"user-123\")\n",
        "print(r2[\"messages\"][-1].content)\n",
        "\n",
        "r3 = invoke_with_memory(product_QnA_agent, \"Recommend 2 models for travel.\", \"user-123\")\n",
        "print(r3[\"messages\"][-1].content)\n"
      ],
      "metadata": {
        "id": "ng7tiB4cO6zF",
        "outputId": "df3645e8-2226-433c-849e-435368bfa73e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but none of the laptops I have information on are under $800. Would you like to know the prices of the available laptops?\n",
            "The AlphaBook Pro and GammaAir X are both excellent choices if you prefer something lightweight and portable.\n",
            "\n",
            "The **AlphaBook Pro** is a sleek ultrabook known for its portability.\n",
            "The **GammaAir X** features a thin and light form factor, making it perfect for users who need high performance in a portable design.\n",
            "Based on your need for travel-friendly laptops, I recommend the **AlphaBook Pro** and the **GammaAir X**.\n",
            "\n",
            "The **AlphaBook Pro** is an ultrabook that offers a great balance of power and portability, making it ideal for professionals on the go.\n",
            "\n",
            "The **GammaAir X** features a thin and light design, perfect for users who require high performance in a portable package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langgraph.prebuilt import create_react_agent\n",
        "# from langgraph.checkpoint.memory import MemorySaver\n",
        "# from langchain_core.messages import AIMessage,HumanMessage,SystemMessage\n",
        "\n",
        "# #Create a System prompt to provide a persona to the chatbot\n",
        "# system_prompt = SystemMessage(\"\"\"\n",
        "#     You are professional chatbot that answers questions about laptops sold by your company.\n",
        "#     To answer questions about laptops, you will ONLY use the available tools and NOT your own memory.\n",
        "#     You will handle small talk and greetings by producing professional responses.\n",
        "#     \"\"\"\n",
        "# )\n",
        "\n",
        "# #Create a list of tools available\n",
        "# tools = [get_laptop_price, get_product_features]\n",
        "\n",
        "# #Create memory across questions in a conversation (conversation memory)\n",
        "# checkpointer=MemorySaver()\n",
        "\n",
        "# #Create a Product QnA Agent. This is actual a graph in langGraph\n",
        "# product_QnA_agent=create_react_agent(\n",
        "#                                 model=model, #LLM to use\n",
        "#                                 tools=tools, #List of tools to use\n",
        "#                                 state_modifier=system_prompt, #The system prompt\n",
        "#                                 debug=False, #Debugging turned on if needed\n",
        "#                                 checkpointer=checkpointer #For conversation memory\n",
        "# )"
      ],
      "metadata": {
        "id": "9WvLKLAk-sid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup chatbot\n",
        "import uuid\n",
        "#To maintain memory, each request should be in the context of a thread.\n",
        "#Each user conversation will use a separate thread ID\n",
        "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
        "\n",
        "#Test the agent with an input\n",
        "from langchain_core.messages import AIMessage,HumanMessage,SystemMessage # Import HumanMessage and AIMessage\n",
        "inputs = {\"messages\":[\n",
        "                HumanMessage(\"What are the features and pricing for GammaAir?\")\n",
        "            ]}\n",
        "\n",
        "#Use streaming to print responses as the agent  does the work.\n",
        "#This is an alternate way to stream agent responses without waiting for the agent to finish\n",
        "for stream in product_QnA_agent.stream(inputs, config, stream_mode=\"values\"):\n",
        "    message=stream[\"messages\"][-1]\n",
        "    if isinstance(message, tuple):\n",
        "        print(message)\n",
        "    else:\n",
        "        message.pretty_print()"
      ],
      "metadata": {
        "id": "vPTag8zC_Fwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0354940c-55aa-4324-a68d-01493b3f1531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What are the features and pricing for GammaAir?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Get_Product_Features (ac78219d-f40d-4671-8721-7fa5e6747e47)\n",
            " Call ID: ac78219d-f40d-4671-8721-7fa5e6747e47\n",
            "  Args:\n",
            "    query: GammaAir\n",
            "  get_laptop_price (530ffef2-4a15-4540-af54-30172c8bc07e)\n",
            " Call ID: 530ffef2-4a15-4540-af54-30172c8bc07e\n",
            "  Args:\n",
            "    laptop_name: GammaAir\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_laptop_price\n",
            "\n",
            "1399\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The GammaAir X features an AMD Ryzen 7 processor with 32GB of DDR4 memory and a 512GB NVMe SSD. It has a thin and light design, making it ideal for users who need high performance in a portable form factor. The price for the GammaAir X is $1399.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execute the Product QnA Chatbot"
      ],
      "metadata": {
        "id": "CQw6ha0h_JXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "#Send a sequence of messages to chatbot and get its response\n",
        "#This simulates the conversation between the user and the Agentic chatbot\n",
        "user_inputs = [\n",
        "    \"Hello\",\n",
        "    \"I am looking to buy a laptop\",\n",
        "    \"Give me a list of available laptop names\",\n",
        "    \"Tell me about the features of  SpectraBook\",\n",
        "    \"How much does it cost?\",\n",
        "    \"Give me similar information about OmegaPro\",\n",
        "    \"What info do you have on AcmeRight ?\",\n",
        "    \"Thanks for the help\"\n",
        "]\n",
        "\n",
        "#Create a new thread\n",
        "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "\n",
        "for input in user_inputs:\n",
        "    print(f\"----------------------------------------\\nUSER : {input}\")\n",
        "    #Format the user message\n",
        "    user_message = {\"messages\":[HumanMessage(input)]}\n",
        "    #Get response from the agent\n",
        "    ai_response = product_QnA_agent.invoke(user_message,config=config)\n",
        "    #Print the response\n",
        "    print(f\"AGENT : {ai_response['messages'][-1].content}\")"
      ],
      "metadata": {
        "id": "oqjlCo8g_HLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eac541-e97b-4162-fc2d-14e080d7fc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "USER : Hello\n",
            "AGENT : Hello! I'm here to help you with any questions you have about our laptops. What can I assist you with today?\n",
            "----------------------------------------\n",
            "USER : I am looking to buy a laptop\n",
            "AGENT : Great! We have several models available:\n",
            "\n",
            "*   **AlphaBook Pro:** A sleek ultrabook with a 12th Gen Intel i7 processor, 16GB of DDR4 RAM, and a fast 1TB SSD. Ideal for professionals on the go.\n",
            "*   **GammaAir X:** Features an AMD Ryzen 7 processor with 32GB of DDR4 memory and a 512GB NVMe SSD. Thin and light, perfect for portable high performance.\n",
            "*   **SpectraBook S:** A workstation-class laptop with an Intel Core i9 processor, 64GB RAM, and a massive 2TB SSD. Designed for intensive tasks like video editing and 3D rendering.\n",
            "*   **OmegaPro G17:** A gaming powerhouse with a Ryzen 9 5900HX CPU, 32GB RAM, and a 1TB SSD. Features a 17-inch display with a high refresh rate and powerful graphics.\n",
            "\n",
            "Do any of these sound interesting to you, or would you like to know more about a specific feature?\n",
            "----------------------------------------\n",
            "USER : Give me a list of available laptop names\n",
            "AGENT : We have the following laptops available: AlphaBook Pro, GammaAir X, SpectraBook S, OmegaPro G17, and NanoEdge Flex.\n",
            "----------------------------------------\n",
            "USER : Tell me about the features of  SpectraBook\n",
            "AGENT : The SpectraBook S is designed for power users. It features an Intel Core i9 processor, 64GB RAM, and a 2TB SSD. This workstation-class laptop is ideal for intensive tasks such as video editing and 3D rendering.\n",
            "----------------------------------------\n",
            "USER : How much does it cost?\n",
            "AGENT : Which laptop are you asking about?\n",
            "----------------------------------------\n",
            "USER : Give me similar information about OmegaPro\n",
            "AGENT : The OmegaPro G17 is a gaming laptop featuring a Ryzen 9 5900HX CPU, 32GB RAM, and a 1TB SSD. It's designed for gamers, with a 17-inch display and a high refresh rate graphics card.\n",
            "\n",
            "The NanoEdge Flex is also listed in the OmegaPro category, but it seems to be a different product. It's a 2-in-1 laptop with Apple's M1 Pro chip, 16GB of unified memory, and a 512GB SSD, aimed at creative professionals.\n",
            "----------------------------------------\n",
            "USER : What info do you have on AcmeRight ?\n",
            "AGENT : I do not have any information on AcmeRight, but I do have information on the OmegaPro G17 and the NanoEdge Flex. The OmegaPro G17 is a gaming laptop with a Ryzen 9 5900HX CPU, 32GB RAM, and a 1TB SSD. The NanoEdge Flex is a 2-in-1 laptop with Apple's M1 Pro chip, 16GB of unified memory, and a 512GB SSD. \n",
            "----------------------------------------\n",
            "USER : Thanks for the help\n",
            "AGENT : You're welcome! Is there anything else I can help you with regarding laptops today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conversation memory by user\n",
        "def execute_prompt(user, config, prompt):\n",
        "    inputs = {\"messages\":[(\"user\",prompt)]}\n",
        "    ai_response = product_QnA_agent.invoke(inputs,config=config)\n",
        "    print(f\"\\n{user}: {ai_response['messages'][-1].content}\")\n",
        "\n",
        "#Create different session threads for 2 users\n",
        "config_1 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "config_2 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "\n",
        "#Test both threads\n",
        "execute_prompt(\"USER 1\", config_1, \"Tell me about the features of  SpectraBook\")\n",
        "execute_prompt(\"USER 2\", config_2, \"Tell me about the features of  GammaAir\")\n",
        "execute_prompt(\"USER 1\", config_1, \"What is its price ?\")\n",
        "execute_prompt(\"USER 2\", config_2, \"What is its price ?\")"
      ],
      "metadata": {
        "id": "Y3jPKPPf_Mzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49cccb3-89ab-4cf4-dbd7-2d92b4cc5cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "USER 1: The SpectraBook S is designed for power users, featuring an Intel Core i9 processor, 64GB RAM, and a massive 2TB SSD. It's a workstation-class laptop, ideal for intensive tasks like video editing and 3D rendering.\n",
            "\n",
            "USER 2: GammaAir X combines an AMD Ryzen 7 processor with 32GB of DDR4 memory and a 512GB NVMe SSD. Its thin and light form factor makes it perfect for users who need high performance in a portable design.\n",
            "\n",
            "USER 1: Please tell me which laptop you are interested in.\n",
            "\n",
            "USER 2: Please tell me which laptop you are interested in.\n"
          ]
        }
      ]
    }
  ]
}